{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOge8XNmiVyQPeR/lTtOU+h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"WEXl-AH9jK1b"},"outputs":[],"source":["from ai_agent import Agent\n","import numpy as np\n","import gymnasium as gym\n","import matplotlib.pyplot as plt\n","import traceback\n","import sys\n","import startups_AI_game as sg\n","import startups_RL_environment as sr\n","import pandas as pd\n","from datetime import datetime\n","import time\n","\n","def plotLearning(x, scores, eps_history, filename):\n","    print(\"Creating plot...\")\n","    try:\n","        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n","\n","        # Plot scores\n","        ax1.plot(x, scores, 'b-', alpha=0.7, label='Score')\n","        ax1.set_xlabel('Episode')\n","        ax1.set_ylabel('Score')\n","        ax1.set_title('Training Scores')\n","        ax1.grid(True)\n","        ax1.legend()\n","\n","        # Plot epsilon values\n","        ax2.plot(x, eps_history, 'r-', alpha=0.7, label='Epsilon')\n","        ax2.set_xlabel('Episode')\n","        ax2.set_ylabel('Epsilon')\n","        ax2.set_title('Epsilon Decay')\n","        ax2.grid(True)\n","        ax2.legend()\n","\n","        plt.tight_layout()\n","        plt.savefig(filename, dpi=300, bbox_inches='tight')\n","        print(f\"Plot saved as {filename}\")\n","        plt.show()\n","    except Exception as e:\n","        print(f\"Error in plotting: {e}\")\n","        traceback.print_exc()\n","\n","print(\"Script starting...\")\n","\n","\n","\n","if __name__ == '__main__':\n","    print(\"Main block entered\")\n","    default_companies = [[\"Giraffe Beer\", 5],[\"Bowwow Games\",6],[\"Flamingo Soft\",7],[\"Octo Coffee\", 8],[\"Hippo Powertech\", 9],[\"Elephant Mars Travel\", 10]]\n","    player_actions_pick_up = [\"pickup_deck\", \"pickup_market\"]\n","    player_actions_put_down = [\"putdown_shares\", \"putdown_market\"]\n","\n","    try:\n","        print(\"Creating environment...\")\n","        env = sr.StartupsEnv(total_players=4, num_humans=0, default_company_list=default_companies)\n","        print(f\"Environment created successfully. Action space: {env.action_space}, Observation space: {env.observation_space}\")\n","\n","\n","        print(\"Creating agent...\")\n","        # make input_dims match the observation space without hardcoding\n","        agent = Agent(alpha=0.0005, gamma=0.99, n_actions=env.action_space.n, epsilon=1.0, batch_size=64, input_dims=env.observation_space.shape[0], epsilon_dec=0.996, epsilon_end=0.01, mem_size=1000000, fname='C:\\\\Users\\\\jezkn\\\\OneDrive\\\\Documents\\\\Startups\\\\StartupsGame\\\\startup_model4.keras')\n","        print(\"Agent created successfully\")\n","\n","        game_history = []\n","        action_history = []\n","\n","        scores = []\n","        eps_history = []\n","        num_episodes = 5000\n","\n","        for i in range(num_episodes):\n","            print(f\"Starting episode {i}\")\n","            done = False\n","            score = 0\n","            episode_start_time = time.time()  # Add this line\n","\n","            try:\n","                observation, info = env.reset()\n","                print(f\"Episode {i} - Initial observation shape: {observation.shape}\")\n","            except Exception as e:\n","                print(f\"Error during env.reset(): {e}\")\n","                traceback.print_exc()\n","                continue\n","\n","            step_count = 0\n","            rl_actions_taken = 0\n","            max_steps = 50\n","\n","            while not done and step_count < max_steps:\n","                try:\n","                    current_phase = env.state_controller.get_current_phase()\n","                    print(f\"Step {step_count}, Phase: {current_phase}\")\n","\n","                    if current_phase in (sr.TurnPhase.RL_PUTDOWN, sr.TurnPhase.RL_PICKUP):\n","                        action = agent.choose_action(observation, env)\n","                        print(f\"RL agent choosing action {action}\")\n","                        rl_actions_taken += 1\n","\n","                        observation_, reward, terminated, truncated, info = env.step(action)\n","                        done = terminated or truncated\n","\n","                        if 'invalid_action' in info and info['invalid_action']:\n","                            print(f\"Invalid action taken: {action}\")\n","\n","                        agent.remember(observation, action, reward, observation_, done)\n","                        observation = observation_\n","                        score += reward\n","\n","                        if agent.memory.mem_cntr > agent.batch_size:\n","                            agent.learn()\n","\n","                        print(f\"Action {action}, Reward: {reward}, Score: {score}\")\n","\n","                    else:\n","                        # Not RL agent's turn - step anyway to advance game state\n","                        # Pass a dummy action (0) since other players will be handled internally\n","                        observation_, reward, terminated, truncated, info = env.step(0)\n","                        done = terminated or truncated\n","                        observation = observation_\n","                        print(f\"Other players' turn, game state advanced\")\n","\n","                    step_count += 1\n","\n","                except Exception as e:\n","                    print(f\"Error during step {step_count} of episode {i}: {e}\")\n","                    traceback.print_exc()\n","                    break\n","\n","            if step_count >= max_steps:\n","                print(f\"Episode {i} exceeded {max_steps} steps, ending...\")\n","\n","            episode_runtime = time.time() - episode_start_time\n","            game_history.append({\n","                'episode': i,\n","                'score': score,\n","                'runtime': episode_runtime,\n","                'steps': step_count,\n","                'rl_actions': rl_actions_taken,\n","                'epsilon': agent.epsilon,\n","                'rl_rank': env._calculate_player_rank()+1,\n","                'rl_coins': env._get_coins_for_score()\n","            })\n","            eps_history.append(agent.epsilon)\n","            scores.append(score)\n","\n","            avg_score = np.mean(scores[max(0, i-100):i+1])\n","            print(f'episode {i}, score {score:.2f}, average score {avg_score:.2f}, epsilon {agent.epsilon:.3f}')\n","\n","            if i % 100 == 0 and i > 0:\n","                try:\n","                    agent.save_model()\n","                    print(f\"Model saved at episode {i}\")\n","                except Exception as e:\n","                    print(f\"Error saving model: {e}\")\n","\n","        # Final model save\n","        try:\n","            agent.save_model()\n","            print(\"Final model saved\")\n","        except Exception as e:\n","            print(f\"Error saving final model: {e}\")\n","\n","        # After training loop, before plotting\n","        print(\"Saving game history...\")\n","        pd.DataFrame(game_history).to_csv('C:\\\\Users\\\\jezkn\\\\OneDrive\\\\Documents\\\\Startups\\\\game_history.csv', index=False)\n","        print(f\"Saved history for {len(game_history)} episodes\")\n","\n","        print(\"Training completed, creating plot...\")\n","        filename = 'C:\\\\Users\\\\jezkn\\\\OneDrive\\\\Documents\\\\Startups\\\\startups_plot.png'\n","        x = [i+1 for i in range(len(scores))]\n","        plotLearning(x, scores, eps_history, filename)\n","        print(\"Script completed successfully\")\n","\n","    except Exception as e:\n","        print(f\"Error occurred: {e}\")\n","        traceback.print_exc()\n","        sys.exit(1)\n","\n","print(\"Script finished\")"]}]}